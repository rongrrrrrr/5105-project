import os
import re
import json
import logging
from typing import Dict, List
# ================= æ—¥å¿—è®¾ç½® =================
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
log_dir = os.path.join(BASE_DIR, "log")
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, "pdf_clean_text_blocks.log")
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_file, encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


import re

COMMON_ABBR = {
    'mr.', 'mrs.', 'ms.', 'dr.', 'prof.', 'sr.', 'jr.',
    'i.e.', 'e.g.', 'etc.', 'vs.', 'viz.',
    'a.m.', 'p.m.', 'jan.', 'feb.', 'mar.', 'apr.', 'jun.', 'jul.',
    'aug.', 'sep.', 'oct.', 'nov.', 'dec.'
}

def split_sentences(text: str):
    """å°å†™/ç¼©å†™ä¿æŠ¤çš„ç®€æ˜“åˆ†å¥ï¼Œè¿”å› List[str]ã€‚"""
    words = text.replace('\n', ' ').split()
    sentences, cur = [], []
    for i, w in enumerate(words):
        cur.append(w)
        if w.endswith(('.', '!', '?')):
            wl = w.lower()
            is_abbr = (
                wl in COMMON_ABBR or
                (len(wl) <= 2 and wl.endswith('.')) or
                re.match(r'\d+\.', wl) or
                re.match(r'[a-z]\.', wl)
            )
            if not is_abbr or i == len(words)-1:
                sentences.append(' '.join(cur).strip())
                cur = []
    if cur:
        sentences.append(' '.join(cur).strip())
    return sentences

# ---------- å•ä½æ˜ å°„ ----------
def load_unit_map(path: str) -> Dict[str, str]:
    if not os.path.exists(path):
        logger.warning(f"unit map not found -> {path}")
        return {}
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    # é•¿åº¦å€’åºï¼Œæœ€é•¿ä¼˜å…ˆ
    return dict(sorted(data.items(), key=lambda x: -len(x[0])))

UNIT_MAP = load_unit_map(os.path.join(BASE_DIR, "data", "esg_unit_mapping", "unit_formatting_dict.json"))

# ================= æ¸…æ´—å‡½æ•° =================
def clean_text_block(text: str) -> str:
    text = text.lstrip('\ufeff')
    text = re.sub(r'[\x00-\x1F\x7F]', '', text)
    text = re.sub(r'(?m)^ç¬¬\s*\d+\s*é¡µ$', '', text)
    text = re.sub(r'(\w)-\n(\w)', r'\1\2', text)

    # ---------- â‘  ä¿®å¤ç¼ºå¤±ç©ºæ ¼ ----------
    # a) å°å†™åè·Ÿå¤§å†™ï¼šlivesour â†’ lives our
    text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)
    # b) å­—æ¯åè·Ÿæ•°å­—ï¼šscope1 â†’ scope 1
    text = re.sub(r'(?<=[A-Za-z])(?=[0-9])', ' ', text)
    # c) æ•°å­—åè·Ÿå­—æ¯ï¼š2023Overview â†’ 2023 Overview
    text = re.sub(r'(?<=[0-9])(?=[A-Za-z])', ' ', text)
    # d) è¿ç€çš„å…¨å¤§å†™ç¼©å†™ä¸å°å†™ï¼šCEOaround â†’ CEO around
    # CEOaround  â†’  CEO around
    text = re.sub(r'([A-Z]{2,})([A-Z][a-z])', r'\1 \2', text)

    # æ®µå†…åˆå¹¶æ¢è¡Œ
    parts = re.split(r'\n{2,}', text)
    cleaned_parts = []
    for part in parts:
        p = part.replace('\n', ' ')
        p = re.sub(r' +', ' ', p)
        cleaned_parts.append(p.strip())
    text = '\n\n'.join(cleaned_parts)

    # æ›¿æ¢æ ‡ç‚¹ã€å»é™¤æ— æ„ä¹‰å­—ç¬¦
    text = re.sub(r'[â˜…â—†â—]', '', text)
    replacements = {
        'â€œ': '"', 'â€': '"', 'â€˜': "'", 'â€™': "'",
        'â€”': '-', 'â€“': '-', 'â€•': '-', 'â€¦': '...'
    }
    for orig, repl in replacements.items():
        text = text.replace(orig, repl)
        # ---------- â¬‡ å•ä½ç»Ÿä¸€ â¬‡ ----------
    for old_unit, new_unit in UNIT_MAP.items():
        text = re.sub(r'\b' + re.escape(old_unit) + r'\b', new_unit, text, flags=re.IGNORECASE)
    # ---------- â‘£ åˆ†å¥ ----------
    sentences = split_sentences(text.strip())   # â† å…ˆåˆ‡æˆå¥å­åˆ—è¡¨
    return "\n".join(sentences)                 # â† æ¯å¥ä¸€è¡Œåè¿”å›

# ================= ä¸»æ§å‡½æ•° =================
def esg_clean_text(extract_text_path):
    with open(extract_text_path, 'r', encoding='utf-8') as f:
        raw_blocks = json.load(f)

    cleaned_blocks = []
    for entry in raw_blocks:
        cleaned = clean_text_block(entry["content"])
        if cleaned:
            cleaned_blocks.append({
                "page": entry["page"],
                "source": entry["source"],
                "content": cleaned
            })

    # è¾“å‡ºè·¯å¾„
    filename = os.path.basename(extract_text_path)
    cleaned_dir = os.path.join(BASE_DIR, "data", "esg_cleaned_text_json")
    os.makedirs(cleaned_dir, exist_ok=True)
    clean_text_path = os.path.join(cleaned_dir, f"cleaned_{filename}")

    with open(clean_text_path, "w", encoding="utf-8") as f:
        json.dump(cleaned_blocks, f, ensure_ascii=False, indent=2)

    logging.info(f"[âœ…] æ¸…æ´—å®Œæˆï¼š{filename}")
    logging.info(f"[ğŸ“] è¾“å‡ºè·¯å¾„ï¼š{clean_text_path}")
    return clean_text_path
